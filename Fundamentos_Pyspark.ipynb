{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1d-mWph7r144QhyQ-0P1qhAbJoLip9Ljx",
      "authorship_tag": "ABX9TyNJpZwF6AWfoNuz4qs6aM/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregorylira/learning-pyspark/blob/main/Fundamentos_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Spark?\n",
        "\n",
        "Spark is a platform for cluster computing. Spark allows you to spread data and perform calculations in clusters with several nodes (in other words, as if there were several different computers doing the calculations). Splitting your data makes working with very large data sets easier because each node only works with a small amount of data.\n",
        "\n",
        "Deciding whether or not Spark is the best solution for your problem requires some experience, but you can consider questions such as:\n",
        "\n",
        "- Is my data too big to run on a single machine?\n",
        "- Can my calculations be easily parallelized?"
      ],
      "metadata": {
        "id": "Yjw2QyYAjptA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "install pyspark on colab"
      ],
      "metadata": {
        "id": "OOcTGwwfkgvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s_6kX2J6BpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99293fd7-29e1-4cff-8d14-2ca09e16afa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=bdfce189ec36dad7a85704550203887fbbe438a7a91e687e6497a13a265eeab0\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mounting the drive to get all data csvs"
      ],
      "metadata": {
        "id": "ao9yG86ckk7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZh6U-bfjOKn",
        "outputId": "9cb59434-25ef-4e6c-b285-5c5d68231aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# creating a local session of pyspark\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .master('local') # create a local instance\n",
        "    .appName(\"learning_pyspark_01\")\n",
        "    .getOrCreate()\n",
        "    )"
      ],
      "metadata": {
        "id": "CW1wATFyk1QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataframe\n",
        "\n",
        "Spark's main data structure is the Resilient Distributed Dataset (RDD). This is a low-level object that allows Spark to work its magic by splitting data across multiple nodes in the cluster. However, RDDs are difficult to work with directly.\n",
        "\n",
        "in this case i using Spark DataFrame\n",
        "\n",
        "The Spark DataFrame is designed to behave much like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, but DataFrames are also more optimized for complicated operations than RDDs.\n",
        "\n",
        "When you start modifying and combining columns and rows of data, there are many ways to achieve the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the correct way to optimize the query, but the DataFrame implementation has a lot of this optimization built in\n",
        "\n",
        "To start working with Spark DataFrames, you first need to create a SparkSession object from your SparkContext. You can think of SparkContext as your connection to the cluster and SparkSession as your interface to that connection.\n",
        "\n",
        "## how visualization table\n",
        "\n",
        "Your SparkSession has an attribute called catalog that lists all the data within the cluster. This attribute has some methods to extract different information.\n",
        "\n",
        "One of the most useful is the .listTables() method, which returns the names of all tables in your cluster as a list."
      ],
      "metadata": {
        "id": "F08gfekKp6ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.catalog.listTables())\n",
        "#in this case, the list is empty because i not load a dataframe in cluster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDP0ejnNmUrh",
        "outputId": "564ca231-8838-486a-fddc-baa1e8266447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import table and make query\n",
        "\n",
        "One of the advantages of the DataFrame interface is that you can run SQL queries against the tables in your Spark cluster\n",
        "\n",
        "in this session i loading a flight.csv. This table contains a row for every flight that left Portland International Airport (PDX) or Seattle-Tacoma International Airport (SEA) in 2014 and 2015."
      ],
      "metadata": {
        "id": "j3MCpA-PsEjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flight_path = \"./drive/MyDrive/learning/spark/flights_small.csv\"\n",
        "flights = spark\\\n",
        "        .read.format(\"csv\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\", \"True\")\\\n",
        "        .csv(flight_path)\n",
        "# use inferSchema to infer type of column, if wrong use: withColumn(\"NewNameColumn\", col(\"ColumnWrongType\").cast(\"integer\")).drop(\"ColumnWrongType\")"
      ],
      "metadata": {
        "id": "PpwA2Op7rlbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((flights.count(), len(flights.columns))) # shape of Dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAg14tv3n66Z",
        "outputId": "38e38b6c-5f19-4116-8c46-2a52f60ce1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flights.show(10) # show 10 first lines of Dataframe in pandas this function is head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujdPSSCVoKv5",
        "outputId": "e650ddaf-c6e4-4f5a-80f6-402b8a84774a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
            "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
            "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
            "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
            "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
            "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
            "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
            "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
            "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
            "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
            "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
            "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flights.printSchema() # show infos of each columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVGYnzu3oSJv",
        "outputId": "6c2d8520-d516-4db3-e8c5-65f0b0b49355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- dep_time: string (nullable = true)\n",
            " |-- dep_delay: string (nullable = true)\n",
            " |-- arr_time: string (nullable = true)\n",
            " |-- arr_delay: string (nullable = true)\n",
            " |-- carrier: string (nullable = true)\n",
            " |-- tailnum: string (nullable = true)\n",
            " |-- flight: integer (nullable = true)\n",
            " |-- origin: string (nullable = true)\n",
            " |-- dest: string (nullable = true)\n",
            " |-- air_time: string (nullable = true)\n",
            " |-- distance: integer (nullable = true)\n",
            " |-- hour: string (nullable = true)\n",
            " |-- minute: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "xXmbZtT9oSVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights = flights.\\\n",
        "        withColumn(\"new_air_time\", col(\"air_time\").cast(\"integer\")).drop(\"air_time\") # this column has wrong type, using cast to tranform in type int\n",
        "\n",
        "# but why create another column ?, in the spark the dataframe is imutable,\n",
        "#in this query \"changes\" are assigned to the flights variable, creating another corrected Dataframe"
      ],
      "metadata": {
        "id": "cqwdODZnochU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights = flights.withColumnRenamed(\"new_air_time\",\"air_time\") # rechange column new name to old name"
      ],
      "metadata": {
        "id": "I7JW6JqapQ3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights.createOrReplaceTempView(\"flights\") # register the dataframe in the temp view to using SQL or SparkSql and this dataframe now is visible in catalog\n",
        "\n",
        "query = \"FROM flights SELECT * LIMIT 10\" # query sql\n",
        "\n",
        "flights10 = spark.sql(query)\n",
        "\n",
        "flights10.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uo8K8CwpfKo",
        "outputId": "a9cbb038-fd09-448d-fc4c-99b8c34d3253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+----+------+--------+\n",
            "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|distance|hour|minute|air_time|\n",
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+----+------+--------+\n",
            "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     954|   6|    58|     132|\n",
            "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|    2677|  10|    40|     360|\n",
            "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     679|  14|    43|     111|\n",
            "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|     569|  17|     5|      83|\n",
            "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     937|   7|    54|     127|\n",
            "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     991|  10|    37|     121|\n",
            "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|     543|   8|    47|      90|\n",
            "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|     679|  16|    55|      98|\n",
            "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|    1050|  12|    36|     135|\n",
            "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|    1721|  18|    12|     198|\n",
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+----+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporary views (like the ones we created above) in Spark SQL are session scoped and will disappear if the session that created them is terminated. If you want to have a temporary view shared across all sessions and keep it active until the Spark application is terminated, you can create a global temporary view. The global temp view is linked to a system-preserved global_temp database, and we must use the qualified name to reference it."
      ],
      "metadata": {
        "id": "QqPDGQ5UAieK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flights.createGlobalTempView(\"flights\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM global_temp.flights LIMIT 10\").show()"
      ],
      "metadata": {
        "id": "bUSZVFTZpp5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GiSoY8BVAoaE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}