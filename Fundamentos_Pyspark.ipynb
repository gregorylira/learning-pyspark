{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1d-mWph7r144QhyQ-0P1qhAbJoLip9Ljx",
      "authorship_tag": "ABX9TyNUdasCvAPcx4iQtM7TG/eu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gregorylira/learning-pyspark/blob/main/Fundamentos_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Spark?\n",
        "\n",
        "Spark is a platform for cluster computing. Spark allows you to spread data and perform calculations in clusters with several nodes (in other words, as if there were several different computers doing the calculations). Splitting your data makes working with very large data sets easier because each node only works with a small amount of data.\n",
        "\n",
        "Deciding whether or not Spark is the best solution for your problem requires some experience, but you can consider questions such as:\n",
        "\n",
        "- Is my data too big to run on a single machine?\n",
        "- Can my calculations be easily parallelized?"
      ],
      "metadata": {
        "id": "Yjw2QyYAjptA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "install pyspark on colab"
      ],
      "metadata": {
        "id": "OOcTGwwfkgvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s_6kX2J6BpC"
      },
      "outputs": [],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mounting the drive to get all data csvs"
      ],
      "metadata": {
        "id": "ao9yG86ckk7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZh6U-bfjOKn",
        "outputId": "d6cfa6aa-c76a-4d88-f869-6f6b06b4cc41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# creating a local session of pyspark\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .master('local') # create a local instance\n",
        "    .appName(\"learning_pyspark_01\")\n",
        "    .getOrCreate()\n",
        "    )"
      ],
      "metadata": {
        "id": "CW1wATFyk1QO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataframe\n",
        "\n",
        "Spark's main data structure is the Resilient Distributed Dataset (RDD). This is a low-level object that allows Spark to work its magic by splitting data across multiple nodes in the cluster. However, RDDs are difficult to work with directly.\n",
        "\n",
        "in this case i using Spark DataFrame\n",
        "\n",
        "The Spark DataFrame is designed to behave much like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, but DataFrames are also more optimized for complicated operations than RDDs.\n",
        "\n",
        "When you start modifying and combining columns and rows of data, there are many ways to achieve the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the correct way to optimize the query, but the DataFrame implementation has a lot of this optimization built in\n",
        "\n",
        "To start working with Spark DataFrames, you first need to create a SparkSession object from your SparkContext. You can think of SparkContext as your connection to the cluster and SparkSession as your interface to that connection.\n",
        "\n",
        "## how visualization table\n",
        "\n",
        "Your SparkSession has an attribute called catalog that lists all the data within the cluster. This attribute has some methods to extract different information.\n",
        "\n",
        "One of the most useful is the .listTables() method, which returns the names of all tables in your cluster as a list."
      ],
      "metadata": {
        "id": "F08gfekKp6ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.catalog.listTables())\n",
        "#in this case, the list is empty because i not load a dataframe in cluster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDP0ejnNmUrh",
        "outputId": "8d5c9b5f-c758-49f9-e7a8-b13b5c7a3995"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import table and make query\n",
        "\n",
        "One of the advantages of the DataFrame interface is that you can run SQL queries against the tables in your Spark cluster\n",
        "\n",
        "in this session i loading a flight.csv. This table contains a row for every flight that left Portland International Airport (PDX) or Seattle-Tacoma International Airport (SEA) in 2014 and 2015."
      ],
      "metadata": {
        "id": "j3MCpA-PsEjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flight_path = \"./drive/MyDrive/learning/spark/flights_small.csv\"\n",
        "flights = spark\\\n",
        "        .read.format(\"csv\")\\\n",
        "        .option(\"inferSchema\", \"True\")\\\n",
        "        .option(\"header\", \"True\")\\\n",
        "        .csv(flight_path)"
      ],
      "metadata": {
        "id": "PpwA2Op7rlbp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}